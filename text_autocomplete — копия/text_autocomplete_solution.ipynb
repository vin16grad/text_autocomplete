{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Ü–µ"
      ],
      "metadata": {
        "id": "S4Xztob6h-OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/text_autocomplete/src/data_utils.py\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "\n",
        "BASE_DIR = \"/content/text_autocomplete\"\n",
        "RAW_PATH_DEFAULT = os.path.join(BASE_DIR, \"data\", \"tweets.txt\")\n",
        "DATA_DIR_DEFAULT = os.path.join(BASE_DIR, \"data\")\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
        "    text = re.sub(r\"@\\w+\", \" \", text)\n",
        "    text = re.sub(r\"[^\\w\\s.,!?]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def prepare_from_txt(raw_path: str = RAW_PATH_DEFAULT,\n",
        "                     data_dir: str = DATA_DIR_DEFAULT,\n",
        "                     train_frac: float = 0.8,\n",
        "                     val_frac: float = 0.1) -> Tuple[str, str, str]:\n",
        "    if not os.path.exists(raw_path):\n",
        "        raise FileNotFoundError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {raw_path}\")\n",
        "\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    # 1) —á–∏—Ç–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π txt\n",
        "    with open(raw_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        lines = [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "    # 2) raw ‚Üí CSV\n",
        "    raw_csv_path = os.path.join(data_dir, \"raw_dataset.csv\")\n",
        "    pd.DataFrame({\"text\": lines}).to_csv(raw_csv_path, index=False)\n",
        "\n",
        "    # 3) —á–∏—Å—Ç–∫–∞\n",
        "    cleaned = [clean_text(t) for t in lines if len(t) > 1]\n",
        "    df = pd.DataFrame({\"text\": cleaned})\n",
        "    proc_path = os.path.join(data_dir, \"dataset_processed.csv\")\n",
        "    df.to_csv(proc_path, index=False)\n",
        "\n",
        "    # 4) —Å–ø–ª–∏—Ç—ã\n",
        "    n = len(df)\n",
        "    n_train = int(n * train_frac)\n",
        "    n_val = int(n * val_frac)\n",
        "    train = df.iloc[:n_train]\n",
        "    val   = df.iloc[n_train:n_train + n_val]\n",
        "    test  = df.iloc[n_train + n_val:]\n",
        "\n",
        "    train_path = os.path.join(data_dir, \"train.csv\")\n",
        "    val_path   = os.path.join(data_dir, \"val.csv\")\n",
        "    test_path  = os.path.join(data_dir, \"test.csv\")\n",
        "\n",
        "    train.to_csv(train_path, index=False)\n",
        "    val.to_csv(val_path, index=False)\n",
        "    test.to_csv(test_path, index=False)\n",
        "\n",
        "    print(f\"‚úî raw ‚Üí {raw_csv_path}\")\n",
        "    print(f\"‚úî processed ‚Üí {proc_path}\")\n",
        "    print(f\"‚úî splits ‚Üí {train_path}, {val_path}, {test_path}\")\n",
        "    print(f\"–†–∞–∑–º–µ—Ä—ã: train={len(train)}, val={len(val)}, test={len(test)}\")\n",
        "    return train_path, val_path, test_path\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prepare_from_txt(RAW_PATH_DEFAULT, DATA_DIR_DEFAULT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCYcwvLOxurB",
        "outputId": "84cf3507-2f07-4d64-9e87-caaa3f0f8104"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/text_autocomplete/src/data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ö–æ–¥–Ω–∏–∫–∞\n",
        "import os\n",
        "print(\"tweets.txt exists:\", os.path.exists(\"/content/text_autocomplete/data/tweets.txt\"))\n",
        "\n",
        "# –∑–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞\n",
        "!python -u /content/text_autocomplete/src/data_utils.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKVpL5zl8aVR",
        "outputId": "b813fc5b-35c0-46e6-fabb-5ae4bf2c1b7e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tweets.txt exists: True\n",
            "‚úî raw ‚Üí /content/text_autocomplete/data/raw_dataset.csv\n",
            "‚úî processed ‚Üí /content/text_autocomplete/data/dataset_processed.csv\n",
            "‚úî splits ‚Üí /content/text_autocomplete/data/train.csv, /content/text_autocomplete/data/val.csv, /content/text_autocomplete/data/test.csv\n",
            "–†–∞–∑–º–µ—Ä—ã: train=1014836, val=126854, test=126855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src/next_token_dataset.py\n",
        "%%writefile /content/text_autocomplete/src/next_token_dataset.py\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "\n",
        "# —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)\n",
        "\n",
        "def tokenize(s: str) -> List[str]:\n",
        "    return TOKEN_RE.findall(str(s).lower())\n",
        "\n",
        "# –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
        "PAD, UNK, BOS, EOS = \"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"\n",
        "\n",
        "# —Å–ª–æ–≤–∞—Ä—å\n",
        "def build_vocab(train_csv: str, min_freq: int = 2, out_dir: str = \"artifacts\"\n",
        "               ) -> Tuple[dict, dict, int, int, int, int]:\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞—ë—Ç —Å–ª–æ–≤–∞—Ä—å –ø–æ train.csv (–∫–æ–ª–æ–Ω–∫–∞ 'text').\n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: stoi, itos, pad_id, unk_id, bos_id, eos_id\n",
        "    \"\"\"\n",
        "    import os\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    df = pd.read_csv(train_csv)\n",
        "    texts = df[\"text\"].astype(str).tolist()\n",
        "\n",
        "    counter = Counter()\n",
        "    for s in texts:\n",
        "        counter.update(tokenize(s))\n",
        "\n",
        "    vocab = [PAD, UNK, BOS, EOS] + [t for t, c in counter.items() if c >= min_freq]\n",
        "    stoi = {t: i for i, t in enumerate(vocab)}\n",
        "    itos = {i: t for t, i in stoi.items()}\n",
        "\n",
        "    with open(f\"{out_dir}/vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(\n",
        "            {\"stoi\": stoi, \"itos\": {str(k): v for k, v in itos.items()}},\n",
        "            f, ensure_ascii=False, indent=2\n",
        "        )\n",
        "\n",
        "    return stoi, itos, stoi[PAD], stoi[UNK], stoi[BOS], stoi[EOS]\n",
        "\n",
        "# –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "def encode(tokens: List[str], stoi: dict, unk_id: int) -> List[int]:\n",
        "    return [stoi.get(t, unk_id) for t in tokens]\n",
        "\n",
        "# —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ä—ã\n",
        "def make_pairs_from_stream(\n",
        "    text_list: List[str],\n",
        "    stoi: dict,\n",
        "    bos_id: int,\n",
        "    eos_id: int,\n",
        "    unk_id: int,\n",
        "    max_len: int = 32\n",
        ") -> List[Tuple[List[int], List[int]]]:\n",
        "    \"\"\"\n",
        "    –±–ª–æ–∫ –¥–∞—ë—Ç x, y —Å–æ —Å–¥–≤–∏–≥–æ–º –Ω–∞ 1\n",
        "    \"\"\"\n",
        "    ids: List[int] = []\n",
        "    for s in text_list:\n",
        "        toks = tokenize(s)\n",
        "        seq  = [bos_id] + encode(toks, stoi, unk_id) + [eos_id]\n",
        "        ids.extend(seq)\n",
        "\n",
        "    pairs: List[Tuple[List[int], List[int]]] = []\n",
        "    for i in range(0, len(ids) - 1, max_len):\n",
        "        x = ids[i : i + max_len]\n",
        "        y = ids[i + 1 : i + 1 + max_len]\n",
        "        if len(x) == len(y):\n",
        "            pairs.append((x, y))\n",
        "    return pairs\n",
        "\n",
        "# Dataset DataLoader\n",
        "class BlockDataset(Dataset):\n",
        "    def __init__(self, pairs: List[Tuple[List[int], List[int]]]):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, i: int):\n",
        "        x, y = self.pairs[i]\n",
        "        return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "def collate_pad(batch, pad_id: int):\n",
        "    xs, ys = list(zip(*batch))\n",
        "    T = max(x.size(0) for x in xs)\n",
        "    xpad = torch.full((len(xs), T), pad_id)\n",
        "    ypad = torch.full((len(xs), T), pad_id)\n",
        "    for i, (x, y) in enumerate(zip(xs, ys)):\n",
        "        xpad[i, : x.size(0)] = x\n",
        "        ypad[i, : y.size(0)] = y\n",
        "    return xpad.long(), ypad.long()\n",
        "\n",
        "def make_loader(\n",
        "    pairs: List[Tuple[List[int], List[int]]],\n",
        "    batch_size: int,\n",
        "    pad_id: int,\n",
        "    shuffle: bool,\n",
        "    pin_memory: bool,\n",
        "    num_workers: int\n",
        ") -> DataLoader:\n",
        "    return DataLoader(\n",
        "        BlockDataset(pairs),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=lambda b: collate_pad(b, pad_id),\n",
        "    )\n",
        "\n",
        "# –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ csv\n",
        "def load_texts(csv_path: str) -> List[str]:\n",
        "    return pd.read_csv(csv_path)[\"text\"].astype(str).tolist()\n",
        "\n",
        "# —Ç–µ—Å—Ç\n",
        "if __name__ == \"__main__\":\n",
        "    # –ø—Ä–∏–º–µ—Ä —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏ ‚Äî –ø–æ–º–µ–Ω—è–π –ø—É—Ç–∏ –ø–æ–¥ —Å–µ–±—è –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏\n",
        "    DATA_DIR = \"/content/text_autocomplete/data\"\n",
        "    train_csv = f\"{DATA_DIR}/train.csv\"\n",
        "\n",
        "    stoi, itos, pad_id, unk_id, bos_id, eos_id = build_vocab(train_csv, min_freq=2, out_dir=\"artifacts\")\n",
        "    texts = load_texts(train_csv)\n",
        "    pairs = make_pairs_from_stream(texts, stoi, bos_id, eos_id, unk_id, max_len=32)\n",
        "\n",
        "    PIN = torch.cuda.is_available()\n",
        "    loader = make_loader(pairs, batch_size=128, pad_id=pad_id, shuffle=True,\n",
        "                         pin_memory=PIN, num_workers=0)\n",
        "\n",
        "    xb, yb = next(iter(loader))\n",
        "    print(\"shapes:\", xb.shape, yb.shape, \"| steps/epoch:\", len(loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1__IcY909k9",
        "outputId": "8ed27b82-c06f-4ed5-805e-5f95c213fe62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/text_autocomplete/src/next_token_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u /content/text_autocomplete/src/next_token_dataset.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37-tyURC-7Kt",
        "outputId": "c76d41ea-1b78-4019-f296-28b30ae79b61"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shapes: torch.Size([128, 32]) torch.Size([128, 32]) | steps/epoch: 4398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM LM\n",
        "%%writefile /content/text_autocomplete/src/lstm_model.py\n",
        "import torch, torch.nn as nn\n",
        "\n",
        "class LSTMLM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb=256, hidden=512, num_layers=2, drop=0.1, pad_id=0):\n",
        "        super().__init__()\n",
        "        self.emb  = nn.Embedding(vocab_size, emb, padding_idx=pad_id)\n",
        "        self.lstm = nn.LSTM(emb, hidden, num_layers=num_layers, batch_first=True, dropout=drop)\n",
        "        self.proj = nn.Linear(hidden, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e = self.emb(x)\n",
        "        h, _ = self.lstm(e)\n",
        "        logits = self.proj(h)\n",
        "        return logits, None\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, prefix_ids, max_new=20, eos=None, device=\"cpu\"):\n",
        "        self.eval()\n",
        "        x = torch.tensor(prefix_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        for _ in range(max_new):\n",
        "            logits, _ = self.forward(x)\n",
        "            next_id = logits[:, -1].argmax(-1)\n",
        "            x = torch.cat([x, next_id.unsqueeze(0)], dim=1)\n",
        "            if eos is not None and int(next_id.item()) == eos:\n",
        "                break\n",
        "        return x.squeeze(0).tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trMoHhO12967",
        "outputId": "66b10361-26ff-44f5-ab4a-05ba853216fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/text_autocomplete/src/lstm_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m py_compile /content/text_autocomplete/src/lstm_model.py\n"
      ],
      "metadata": {
        "id": "kAAFODrTBDTz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/text_autocomplete/src/lstm_train.py\n",
        "# lstm_train.py\n",
        "import os, sys, json, math\n",
        "import torch, torch.nn as nn\n",
        "from tqdm.auto import tqdm\n",
        "import argparse\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "MAX_LEN = 32\n",
        "BATCH_SIZE = 128\n",
        "PIN = torch.cuda.is_available()\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "# –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π\n",
        "try:\n",
        "    HERE = os.path.dirname(os.path.abspath(__file__))   # .../text_autocomplete/src\n",
        "    BASE = os.path.abspath(os.path.join(HERE, \"..\"))    # .../text_autocomplete\n",
        "except NameError:\n",
        "    BASE = \"/content/text_autocomplete\"\n",
        "    HERE = os.path.join(BASE, \"src\")\n",
        "SRC  = os.path.join(BASE, \"src\")\n",
        "if SRC not in sys.path:\n",
        "    sys.path.insert(0, SRC)\n",
        "\n",
        "DATA_DIR    = os.path.join(BASE, \"data\")\n",
        "ART_DIR     = os.path.join(BASE, \"artifacts\")\n",
        "MODEL_DIR   = os.path.join(BASE, \"models\")\n",
        "RESULTS_DIR = os.path.join(BASE, \"results\")\n",
        "\n",
        "from data_utils import prepare_from_txt\n",
        "from next_token_dataset import build_vocab, load_texts, make_pairs_from_stream, make_loader\n",
        "from lstm_model import LSTMLM\n",
        "\n",
        "UNK = \"<unk>\"\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def ids_to_text(ids, itos: dict, pad_id: int):\n",
        "    return \" \".join(itos.get(i, UNK) for i in ids if i != pad_id)\n",
        "\n",
        "def _ngrams(seq, n):\n",
        "    return [\" \".join(seq[i:i+n]) for i in range(len(seq)-n+1)] if len(seq) >= n else []\n",
        "\n",
        "def rouge_f1(pred_tokens, ref_tokens, n):\n",
        "    p_ngr, r_ngr = Counter(_ngrams(pred_tokens, n)), Counter(_ngrams(ref_tokens, n))\n",
        "    overlap = sum((p_ngr & r_ngr).values())\n",
        "    pred_cnt, ref_cnt = max(1, sum(p_ngr.values())), max(1, sum(r_ngr.values()))\n",
        "    prec = overlap / pred_cnt\n",
        "    rec  = overlap / ref_cnt\n",
        "    return 0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_rouge_on_loader(model, loader, itos, pad_id, eos_id, device, take_ratio=0.75, max_batches=None):\n",
        "    model.eval()\n",
        "    r1s, r2s, seen = [], [], 0\n",
        "    for xb, _ in loader:\n",
        "        if max_batches is not None and seen >= max_batches:\n",
        "            break\n",
        "        seen += 1\n",
        "        xb = xb.to(device)\n",
        "        seq = xb[0].tolist()\n",
        "        L = len([t for t in seq if t != pad_id])\n",
        "        k = max(1, int(L * take_ratio))\n",
        "        prefix, ref = seq[:k], seq[k:L]\n",
        "        gen = model.generate(prefix, max_new=len(ref), eos=eos_id, device=device)\n",
        "        pred = gen[k:L]\n",
        "        r1s.append(rouge_f1([itos.get(i, UNK) for i in pred], [itos.get(i, UNK) for i in ref], 1))\n",
        "        r2s.append(rouge_f1([itos.get(i, UNK) for i in pred], [itos.get(i, UNK) for i in ref], 2))\n",
        "    n = max(1, len(r1s))\n",
        "    return float(sum(r1s)/n), float(sum(r2s)/n)\n",
        "\n",
        "def run_epoch_bar(model, loader, criterion, optimizer, scaler, device, pad_id, train=True, desc=\"\"):\n",
        "    model.train(train)\n",
        "    total_loss, total_tok = 0.0, 0\n",
        "    pbar = tqdm(loader, desc=desc)\n",
        "    for xb, yb in pbar:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        with torch.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n",
        "            logits, _ = model(xb)\n",
        "            loss = criterion(logits.reshape(-1, logits.size(-1)), yb.reshape(-1))\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "        with torch.no_grad():\n",
        "            tokens = int((yb != pad_id).sum().item())\n",
        "            total_loss += loss.item() * tokens; total_tok += tokens\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "    return total_loss / max(1, total_tok)\n",
        "\n",
        "# –æ—Å–Ω–æ–≤–∞\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--epochs\", type=int, default=2)         # 1‚Äì3 –æ–∫\n",
        "    ap.add_argument(\"--min_freq\", type=int, default=2)\n",
        "    ap.add_argument(\"--raw_txt\", default=os.path.join(DATA_DIR, \"tweets.txt\"))\n",
        "    args = ap.parse_args([]) if \"ipykernel\" in sys.modules else ap.parse_args()\n",
        "\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "    # –¥–∞–Ω–Ω—ã–µ\n",
        "    train_csv = os.path.join(DATA_DIR, \"train.csv\")\n",
        "    val_csv   = os.path.join(DATA_DIR, \"val.csv\")\n",
        "    test_csv  = os.path.join(DATA_DIR, \"test.csv\")\n",
        "    if not os.path.exists(train_csv):\n",
        "        if not os.path.exists(args.raw_txt):\n",
        "            raise FileNotFoundError(f\"–ù–µ—Ç –∏—Å—Ö–æ–¥–Ω–∏–∫–∞: {args.raw_txt}\")\n",
        "        prepare_from_txt(args.raw_txt, DATA_DIR)\n",
        "\n",
        "    # —Å–ª–æ–≤–∞—Ä—å\n",
        "    stoi, itos, pad_id, unk_id, bos_id, eos_id = build_vocab(train_csv, min_freq=args.min_freq, out_dir=ART_DIR)\n",
        "    vocab_size = len(stoi)\n",
        "\n",
        "    # –ª–æ–∞–¥–µ—Ä—ã\n",
        "    train_texts = load_texts(train_csv)\n",
        "    val_texts   = load_texts(val_csv)\n",
        "    test_texts  = load_texts(test_csv)\n",
        "\n",
        "    train_pairs = make_pairs_from_stream(train_texts, stoi, bos_id, eos_id, unk_id, max_len=MAX_LEN)\n",
        "    val_pairs   = make_pairs_from_stream(val_texts,   stoi, bos_id, eos_id, unk_id, max_len=MAX_LEN)\n",
        "    test_pairs  = make_pairs_from_stream(test_texts,  stoi, bos_id, eos_id, unk_id, max_len=MAX_LEN)\n",
        "\n",
        "    train_loader = make_loader(train_pairs, BATCH_SIZE, pad_id, True,  PIN, NUM_WORKERS)\n",
        "    val_loader   = make_loader(val_pairs,   BATCH_SIZE, pad_id, False, PIN, NUM_WORKERS)\n",
        "    test_loader  = make_loader(test_pairs,  BATCH_SIZE, pad_id, False, PIN, NUM_WORKERS)\n",
        "\n",
        "    # –º–æ–¥–µ–ª—å\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = LSTMLM(vocab_size, emb=256, hidden=512, num_layers=2, drop=0.1, pad_id=pad_id).to(device)\n",
        "    print(f\"{sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=torch.cuda.is_available())\n",
        "\n",
        "    # –ª–æ–≥ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
        "    train_losses, val_losses, ppls = [], [], []\n",
        "    best_val = float(\"inf\")\n",
        "    ckpt = os.path.join(MODEL_DIR, \"lstm.pt\")\n",
        "\n",
        "    for ep in range(1, args.epochs + 1):\n",
        "        tr = run_epoch_bar(model, train_loader, criterion, optimizer, scaler, device, pad_id, True,  f\"Epoch {ep}/{args.epochs} [Train]\")\n",
        "        va = run_epoch_bar(model, val_loader,   criterion, optimizer, scaler, device, pad_id, False, f\"Epoch {ep}/{args.epochs} [Val]  \")\n",
        "        r1, r2 = eval_rouge_on_loader(model, val_loader, itos, pad_id, eos_id, device, take_ratio=0.75, max_batches=200)\n",
        "        ppl = math.exp(va) if va < 20 else float(\"inf\")\n",
        "\n",
        "        train_losses.append(tr); val_losses.append(va); ppls.append(ppl)\n",
        "\n",
        "        xb, _ = next(iter(val_loader))\n",
        "        seq = xb[0].tolist(); L = len([t for t in seq if t != pad_id]); k = max(1, int(L*0.75))\n",
        "        prefix, ref = seq[:k], seq[k:L]\n",
        "        pred = model.generate(prefix, max_new=len(ref), eos=eos_id, device=device)\n",
        "\n",
        "        print(f\"\\nEpoch {ep}: TrainLoss={tr:.4f} | ValLoss={va:.4f} | ValPPL={ppl:.2f} | ROUGE-1={r1:.4f} | ROUGE-2={r2:.4f}\")\n",
        "        print(\"  –í—Ö–æ–¥ (3/4):  \", ids_to_text(prefix, itos, pad_id))\n",
        "        print(\"  –¢–∞—Ä–≥–µ—Ç (1/4):\", ids_to_text(ref,    itos, pad_id))\n",
        "        print(\"  –ú–æ–¥–µ–ª—å (1/4):\", ids_to_text(pred[k:L], itos, pad_id))\n",
        "\n",
        "        if va < best_val:\n",
        "            best_val = va\n",
        "            torch.save(model.state_dict(), ckpt)\n",
        "            print(f\"Saved best to {ckpt}\")\n",
        "\n",
        "    # —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ + —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "    r1_val, r2_val = eval_rouge_on_loader(model, val_loader,  itos, pad_id, eos_id, device, take_ratio=0.75, max_batches=None)\n",
        "    r1_test, r2_test = eval_rouge_on_loader(model, test_loader, itos, pad_id, eos_id, device, take_ratio=0.75, max_batches=None)\n",
        "\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "    with open(os.path.join(RESULTS_DIR, \"lstm_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"val\":{\"rouge1_f1\":r1_val,\"rouge2_f1\":r2_val},\n",
        "                   \"test\":{\"rouge1_f1\":r1_test,\"rouge2_f1\":r2_test}}, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"[VAL]  ROUGE-1={r1_val:.4f} | ROUGE-2={r2_val:.4f}\")\n",
        "    print(f\"[TEST] ROUGE-1={r1_test:.4f} | ROUGE-2={r2_test:.4f}\")\n",
        "    print(f\"–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {os.path.join(RESULTS_DIR, 'lstm_metrics.json')}\")\n",
        "\n",
        "    # –≥—Ä–∞—Ñ–∏–∫–∏ (loss –∏ perplexity)\n",
        "    # loss per epoch\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(train_losses, '-o', label='train')\n",
        "    plt.plot(val_losses,   '-o', label='val')\n",
        "    plt.title('Loss per epoch'); plt.xlabel('epoch'); plt.ylabel('loss')\n",
        "    plt.grid(True); plt.legend()\n",
        "    loss_png = os.path.join(RESULTS_DIR, \"loss.png\")\n",
        "    plt.savefig(loss_png, bbox_inches='tight'); plt.close()\n",
        "\n",
        "    # Val Perplexity\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(ppls, '-o', label='val PPL')\n",
        "    plt.title('Validation Perplexity'); plt.xlabel('epoch'); plt.ylabel('PPL')\n",
        "    plt.grid(True); plt.legend()\n",
        "    ppl_png = os.path.join(RESULTS_DIR, \"ppl.png\")\n",
        "    plt.savefig(ppl_png, bbox_inches='tight'); plt.close()\n",
        "\n",
        "    print(f\"–ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {loss_png} –∏ {ppl_png}\")\n",
        "\n",
        "\n",
        "    if \"ipykernel\" in sys.modules:\n",
        "        from IPython.display import Image, display\n",
        "        display(Image(filename=loss_png))\n",
        "        display(Image(filename=ppl_png))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZWK42cR3nob",
        "outputId": "232282fb-bd6d-418e-fe3f-ab1ca3603fc4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/text_autocomplete/src/lstm_train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u /content/text_autocomplete/src/lstm_train.py --epochs 2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5iDCBmSEEy8",
        "outputId": "6ab9aaeb-0f30-458e-d0a4-5b6b8360ce89"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68,411,090 parameters\n",
            "Epoch 1/2 [Train]: 100% 4398/4398 [11:02<00:00,  6.64it/s, loss=4.8027]\n",
            "Epoch 1/2 [Val]  : 100% 521/521 [00:30<00:00, 17.26it/s, loss=4.9511]\n",
            "\n",
            "Epoch 1: TrainLoss=5.1088 | ValLoss=4.9185 | ValPPL=136.80 | ROUGE-1=0.1310 | ROUGE-2=0.0315\n",
            "  –í—Ö–æ–¥ (3/4):   <bos> just a <unk> full of sugar helps the meds goo down in the most delightful wayyy 39 days ! lt 3 <eos> <bos>\n",
            "  –¢–∞—Ä–≥–µ—Ç (1/4): well my first wedding anniversary on sunday which\n",
            "  –ú–æ–¥–µ–ª—å (1/4): i m so sorry to hear that .\n",
            "‚úÖ Saved best to /content/text_autocomplete/models/lstm.pt\n",
            "Epoch 2/2 [Train]: 100% 4398/4398 [11:04<00:00,  6.62it/s, loss=4.9791]\n",
            "Epoch 2/2 [Val]  : 100% 521/521 [00:30<00:00, 17.28it/s, loss=4.8964]\n",
            "\n",
            "Epoch 2: TrainLoss=4.7897 | ValLoss=4.8577 | ValPPL=128.73 | ROUGE-1=0.1378 | ROUGE-2=0.0266\n",
            "  –í—Ö–æ–¥ (3/4):   <bos> just a <unk> full of sugar helps the meds goo down in the most delightful wayyy 39 days ! lt 3 <eos> <bos>\n",
            "  –¢–∞—Ä–≥–µ—Ç (1/4): well my first wedding anniversary on sunday which\n",
            "  –ú–æ–¥–µ–ª—å (1/4): i m so sorry to hear that .\n",
            "‚úÖ Saved best to /content/text_autocomplete/models/lstm.pt\n",
            "[VAL]  ROUGE-1=0.1428 | ROUGE-2=0.0293\n",
            "[TEST] ROUGE-1=0.1457 | ROUGE-2=0.0279\n",
            "‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ /content/text_autocomplete/results/lstm_metrics.json\n",
            "üìà –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: /content/text_autocomplete/results/loss.png –∏ /content/text_autocomplete/results/ppl.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src/eval_lstm.py\n",
        "%%writefile /content/text_autocomplete/src/eval_lstm.py\n",
        "import os, sys, json, torch\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π\n",
        "try:\n",
        "    HERE = os.path.dirname(os.path.abspath(__file__))   # .../text_autocomplete/src\n",
        "    BASE = os.path.abspath(os.path.join(HERE, \"..\"))    # .../text_autocomplete\n",
        "except NameError:\n",
        "    BASE = \"/content/text_autocomplete\"\n",
        "SRC  = os.path.join(BASE, \"src\")\n",
        "if SRC not in sys.path:\n",
        "    sys.path.insert(0, SRC)\n",
        "\n",
        "DATA_DIR    = os.path.join(BASE, \"data\")\n",
        "ART_DIR     = os.path.join(BASE, \"artifacts\")\n",
        "MODEL_DIR   = os.path.join(BASE, \"models\")\n",
        "RESULTS_DIR = os.path.join(BASE, \"results\")\n",
        "\n",
        "from next_token_dataset import build_vocab, load_texts, make_pairs_from_stream, make_loader\n",
        "from lstm_model import LSTMLM\n",
        "\n",
        "UNK = \"<unk>\"\n",
        "\n",
        "def ids_to_text(ids, itos: dict, pad_id: int):\n",
        "    return \" \".join(itos.get(i, UNK) for i in ids if i != pad_id)\n",
        "\n",
        "def _ngrams(seq, n):\n",
        "    return [\" \".join(seq[i:i+n]) for i in range(len(seq)-n+1)] if len(seq) >= n else []\n",
        "\n",
        "def rouge_f1(pred_tokens, ref_tokens, n):\n",
        "    p_ngr, r_ngr = Counter(_ngrams(pred_tokens, n)), Counter(_ngrams(ref_tokens, n))\n",
        "    overlap = sum((p_ngr & r_ngr).values())\n",
        "    pred_cnt, ref_cnt = max(1, sum(p_ngr.values())), max(1, sum(r_ngr.values()))\n",
        "    prec = overlap / pred_cnt; rec = overlap / ref_cnt\n",
        "    return 0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_rouge_on_loader(model, loader, itos, pad_id, eos_id, device, take_ratio=0.75, max_batches=None):\n",
        "    model.eval()\n",
        "    r1s, r2s, seen = [], [], 0\n",
        "    for xb, _ in loader:\n",
        "        if max_batches is not None and seen >= max_batches:\n",
        "            break\n",
        "        seen += 1\n",
        "        xb = xb.to(device)\n",
        "        seq = xb[0].tolist()\n",
        "        L = len([t for t in seq if t != pad_id])\n",
        "        k = max(1, int(L * take_ratio))\n",
        "        prefix, ref = seq[:k], seq[k:L]\n",
        "        gen = model.generate(prefix, max_new=len(ref), eos=eos_id, device=device)\n",
        "        pred = gen[k:L]\n",
        "        r1s.append(rouge_f1([itos.get(i, UNK) for i in pred], [itos.get(i, UNK) for i in ref], 1))\n",
        "        r2s.append(rouge_f1([itos.get(i, UNK) for i in pred], [itos.get(i, UNK) for i in ref], 2))\n",
        "    n = max(1, len(r1s))\n",
        "    return float(sum(r1s)/n), float(sum(r2s)/n)\n",
        "\n",
        "def main():\n",
        "    # —Ñ–∞–π–ª—ã\n",
        "    train_csv = os.path.join(DATA_DIR, \"train.csv\")\n",
        "    val_csv   = os.path.join(DATA_DIR, \"val.csv\")\n",
        "    test_csv  = os.path.join(DATA_DIR, \"test.csv\")\n",
        "    ckpt = os.path.join(MODEL_DIR, \"lstm.pt\")\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(ckpt):\n",
        "        raise FileNotFoundError(f\"–ù–µ—Ç —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {ckpt}. –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏ –º–æ–¥–µ–ª—å (lstm_train.py).\")\n",
        "\n",
        "    # —Å–ª–æ–≤–∞—Ä—å –∏ data loaders\n",
        "    stoi, itos, pad_id, unk_id, bos_id, eos_id = build_vocab(train_csv, min_freq=2, out_dir=ART_DIR)\n",
        "    MAX_LEN = 32; BS = 128; PIN = torch.cuda.is_available()\n",
        "\n",
        "    val_pairs   = make_pairs_from_stream(load_texts(val_csv),   stoi, bos_id, eos_id, unk_id, max_len=MAX_LEN)\n",
        "    test_pairs  = make_pairs_from_stream(load_texts(test_csv),  stoi, bos_id, eos_id, unk_id, max_len=MAX_LEN)\n",
        "    val_loader  = make_loader(val_pairs,  BS, pad_id, False, PIN, 0)\n",
        "    test_loader = make_loader(test_pairs, BS, pad_id, False, PIN, 0)\n",
        "\n",
        "    # –º–æ–¥–µ–ª—å  –≤–µ—Å–∞\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = LSTMLM(vocab_size=len(stoi), emb=256, hidden=512, num_layers=2, drop=0.1, pad_id=pad_id).to(device)\n",
        "    model.load_state_dict(torch.load(ckpt, map_location=device))\n",
        "\n",
        "    # –º–µ—Ç—Ä–∏–∫–∏\n",
        "    r1_val, r2_val   = eval_rouge_on_loader(model, val_loader,  itos, pad_id, eos_id, device, take_ratio=0.75, max_batches=None)\n",
        "    r1_test, r2_test = eval_rouge_on_loader(model, test_loader, itos, pad_id, eos_id, device, take_ratio=0.75, max_batches=None)\n",
        "\n",
        "    print(f\"[VAL]  ROUGE-1={r1_val:.4f} | ROUGE-2={r2_val:.4f}\")\n",
        "    print(f\"[TEST] ROUGE-1={r1_test:.4f} | ROUGE-2={r2_test:.4f}\")\n",
        "\n",
        "    # –ø—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
        "    xb, _ = next(iter(test_loader))\n",
        "    seq = xb[0].tolist()\n",
        "    L = len([t for t in seq if t != pad_id]); k = max(1, int(L*0.75))\n",
        "    prefix, ref = seq[:k], seq[k:L]\n",
        "    pred = model.generate(prefix, max_new=len(ref), eos=eos_id, device=device)\n",
        "    print(\"\\n–ü–†–ò–ú–ï–† –ì–ï–ù–ï–†–ê–¶–ò–ò (test):\")\n",
        "    print(\"  –í—Ö–æ–¥ (3/4):  \", ids_to_text(prefix, itos, pad_id))\n",
        "    print(\"  –¢–∞—Ä–≥–µ—Ç (1/4):\", ids_to_text(ref,    itos, pad_id))\n",
        "    print(\"  –ú–æ–¥–µ–ª—å (1/4):\", ids_to_text(pred[k:L], itos, pad_id))\n",
        "\n",
        "    # —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ JSON\n",
        "    out_json = os.path.join(RESULTS_DIR, \"lstm_metrics_eval.json\")\n",
        "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"val\":{\"rouge1_f1\":r1_val,\"rouge2_f1\":r2_val},\n",
        "                   \"test\":{\"rouge1_f1\":r1_test,\"rouge2_f1\":r2_test}}, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"\\n–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {out_json}\")\n",
        "\n",
        "    # –≥—Ä–∞—Ñ–∏–∫ ROUGE\n",
        "    plt.figure(figsize=(5,4))\n",
        "    x = [\"Val R1\",\"Val R2\",\"Test R1\",\"Test R2\"]\n",
        "    y = [r1_val, r2_val, r1_test, r2_test]\n",
        "    plt.bar(x, y)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(\"ROUGE (F1)\")\n",
        "    plt.grid(axis=\"y\", alpha=0.3)\n",
        "    rouge_png = os.path.join(RESULTS_DIR, \"rouge_eval.png\")\n",
        "    plt.savefig(rouge_png, bbox_inches=\"tight\"); plt.close()\n",
        "    print(f\"–ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {rouge_png}\")\n",
        "\n",
        "\n",
        "    if \"ipykernel\" in sys.modules:\n",
        "        from IPython.display import Image, display\n",
        "        display(Image(filename=rouge_png))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehyUm0S7939U",
        "outputId": "6592f208-5532-44aa-fd1d-5cf8f098f7d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/text_autocomplete/src/eval_lstm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u /content/text_autocomplete/src/eval_lstm.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px54vm_iX_FJ",
        "outputId": "450eccd5-aa64-4c55-fc75-51a0f89baf68"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL]  ROUGE-1=0.1428 | ROUGE-2=0.0293\n",
            "[TEST] ROUGE-1=0.1457 | ROUGE-2=0.0279\n",
            "\n",
            "–ü–†–ò–ú–ï–† –ì–ï–ù–ï–†–ê–¶–ò–ò (test):\n",
            "  –í—Ö–æ–¥ (3/4):   <bos> point mallard tomorrow morning at 12 waterslides , lifeguards , and sun <unk> . tehe ! <eos> <bos> extremely happy right now !\n",
            "  –¢–∞—Ä–≥–µ—Ç (1/4): you don t even know les <eos> <bos>\n",
            "  –ú–æ–¥–µ–ª—å (1/4): <eos>\n",
            "\n",
            "‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ /content/text_autocomplete/results/lstm_metrics_eval.json\n",
            "üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: /content/text_autocomplete/results/rouge_eval.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src/eval_transformer_pipeline.py\n",
        "%%writefile /content/text_autocomplete/src/eval_transformer_pipeline.py\n",
        "import os, sys, json, random\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# –ø—É—Ç–∏\n",
        "try:\n",
        "    HERE = os.path.dirname(os.path.abspath(__file__))   # .../text_autocomplete/src\n",
        "    BASE = os.path.abspath(os.path.join(HERE, \"..\"))    # .../text_autocomplete\n",
        "except NameError:\n",
        "    BASE = \"/content/text_autocomplete\"\n",
        "SRC  = os.path.join(BASE, \"src\")\n",
        "if SRC not in sys.path:\n",
        "    sys.path.insert(0, SRC)\n",
        "\n",
        "DATA_DIR    = os.path.join(BASE, \"data\")\n",
        "RESULTS_DIR = os.path.join(BASE, \"results\")\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
        "from transformers import pipeline, set_seed\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def load_val_texts(val_csv_path: str, sample_size: int = 100, seed: int = 42):\n",
        "    df = pd.read_csv(val_csv_path)\n",
        "    if \"text\" not in df.columns:\n",
        "        df = df.rename(columns={df.columns[0]: \"text\"})\n",
        "    df = df.dropna(subset=[\"text\"])\n",
        "    df[\"text\"] = df[\"text\"].astype(str)\n",
        "    if len(df) > sample_size:\n",
        "        df = df.sample(sample_size, random_state=seed)\n",
        "    return df[\"text\"].tolist()\n",
        "\n",
        "def build_generator(model_name=\"distilgpt2\", seed=42):\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    set_seed(seed)\n",
        "    gen = pipeline(\n",
        "        task=\"text-generation\",\n",
        "        model=model_name,\n",
        "        device=device\n",
        "    )\n",
        "    return gen\n",
        "\n",
        "def complete_text(gen, prompt, max_new_tokens=30):\n",
        "    out = gen(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        top_k=50,\n",
        "        pad_token_id=gen.model.config.eos_token_id,\n",
        "    )[0][\"generated_text\"]\n",
        "    if out.startswith(prompt):\n",
        "        cont = out[len(prompt):]\n",
        "    else:\n",
        "        cont = out\n",
        "    return cont\n",
        "\n",
        "def main():\n",
        "    # –∫–æ–Ω—Ñ–∏–≥\n",
        "    VAL_CSV = os.path.join(DATA_DIR, \"val.csv\")\n",
        "    MODEL_NAME = \"distilgpt2\"\n",
        "    SAMPLE_SIZE = 100\n",
        "    SEED = 42\n",
        "    CUTOFF_RATIO = 0.75\n",
        "    MAX_NEW_TOKENS = 30\n",
        "\n",
        "    # –¥–∞–Ω–Ω—ã–µ\n",
        "    if not os.path.exists(VAL_CSV):\n",
        "        raise FileNotFoundError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω {VAL_CSV}. –°–Ω–∞—á–∞–ª–∞ –ø–æ–¥–≥–æ—Ç–æ–≤—å —Å–ø–ª–∏—Ç—ã (train/val/test.csv).\")\n",
        "    texts = load_val_texts(VAL_CSV, sample_size=SAMPLE_SIZE, seed=SEED)\n",
        "\n",
        "    # –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä\n",
        "    generator = build_generator(MODEL_NAME, seed=SEED)\n",
        "\n",
        "    # –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ ROUGE\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
        "    r1s, r2s = [], []\n",
        "    samples = []\n",
        "\n",
        "    # —Ü–∏–∫–ª –ø–æ –ø—Ä–∏–º–µ—Ä–∞–º\n",
        "    random.seed(SEED)\n",
        "    for text in texts:\n",
        "        text = text.strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        cutoff = max(1, int(len(text) * CUTOFF_RATIO))\n",
        "        prefix = text[:cutoff]\n",
        "        target = text[cutoff:]\n",
        "        pred_cont = complete_text(generator, prefix, max_new_tokens=MAX_NEW_TOKENS)\n",
        "\n",
        "        # ROUGE –ø–æ —Ç–æ–∫–µ–Ω–∞–º —Å—Ç—Ä–æ–∫–∏\n",
        "        scores = scorer.score(target, pred_cont)\n",
        "        r1s.append(scores[\"rouge1\"].fmeasure)\n",
        "        r2s.append(scores[\"rouge2\"].fmeasure)\n",
        "\n",
        "        # –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø–µ—á–∞—Ç–∏\n",
        "        if len(samples) < 3:\n",
        "            samples.append({\"prefix\": prefix, \"target\": target, \"pred\": pred_cont})\n",
        "\n",
        "    # —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "    r1_mean = float(sum(r1s) / max(1, len(r1s)))\n",
        "    r2_mean = float(sum(r2s) / max(1, len(r2s)))\n",
        "\n",
        "    print(f\"distilgpt2 on val ({len(r1s)} samples)\")\n",
        "    print(f\"ROUGE-1 F1 = {r1_mean:.4f} | ROUGE-2 F1 = {r2_mean:.4f}\")\n",
        "\n",
        "    # –ø—Ä–∏–º–µ—Ä—ã\n",
        "    for i, s in enumerate(samples, 1):\n",
        "        print(f\"\\n–ü—Ä–∏–º–µ—Ä {i}:\")\n",
        "        print(\"  –í—Ö–æ–¥ (3/4): \", s[\"prefix\"])\n",
        "        print(\"  –¢–∞—Ä–≥–µ—Ç (1/4):\", s[\"target\"])\n",
        "        print(\"  –ú–æ–¥–µ–ª—å (1/4):\", s[\"pred\"])\n",
        "\n",
        "    # —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ JSON\n",
        "    out_json = os.path.join(RESULTS_DIR, \"transformer_metrics.json\")\n",
        "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"rouge1_f1\": r1_mean, \"rouge2_f1\": r2_mean}, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"\\n–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {out_json}\")\n",
        "\n",
        "    # –≥—Ä–∞—Ñ–∏–∫ ROUGE\n",
        "    plt.figure(figsize=(5,4))\n",
        "    x = [\"ROUGE-1\", \"ROUGE-2\"]\n",
        "    y = [r1_mean, r2_mean]\n",
        "    plt.bar(x, y)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(\"distilgpt2 on val\")\n",
        "    plt.grid(axis=\"y\", alpha=0.3)\n",
        "    out_png = os.path.join(RESULTS_DIR, \"transformer_rouge.png\")\n",
        "    plt.savefig(out_png, bbox_inches=\"tight\"); plt.close()\n",
        "    print(f\"–ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {out_png}\")\n",
        "\n",
        "    if \"ipykernel\" in sys.modules:\n",
        "        from IPython.display import Image, display\n",
        "        display(Image(filename=out_png))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztd1I1lFYZBO",
        "outputId": "05769847-0cdc-4ddc-c0c0-1db64cf614a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/text_autocomplete/src/eval_transformer_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05MPPtbqZ7n2",
        "outputId": "b6dc1a94-4d2e-43aa-cbcf-9970ef97bf5d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u /content/text_autocomplete/src/eval_transformer_pipeline.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R15MsLknZrU8",
        "outputId": "1dfeac36-6caa-4968-966f-ac473fe8b371"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-09 11:37:53.573345: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762688273.636193   27329 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762688273.682446   27329 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762688273.753553   27329 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762688273.753601   27329 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762688273.753609   27329 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762688273.753616   27329 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-09 11:37:53.768517: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 762/762 [00:00<00:00, 2.09MB/s]\n",
            "model.safetensors: 100% 353M/353M [00:04<00:00, 81.4MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 735kB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 50.0kB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 7.07MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 3.25MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 15.6MB/s]\n",
            "Device set to use cuda:0\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "distilgpt2 on val (100 samples)\n",
            "ROUGE-1 F1 = 0.0334 | ROUGE-2 F1 = 0.0046\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä 1:\n",
            "  –í—Ö–æ–¥ (3/4):  ok, now i m jealous lol! that s so awesome! i m seeing them on july 11th, but i m in the\n",
            "  –¢–∞—Ä–≥–µ—Ç (1/4):  20th row...andy s side though\n",
            "  –ú–æ–¥–µ–ª—å (1/4):  past i didnt have a clue what i was thinking and i didnt see them on july 11th, but i m in the past i didnt have\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä 2:\n",
            "  –í—Ö–æ–¥ (3/4):  thanks for the birthday wishes everyone!! it \n",
            "  –¢–∞—Ä–≥–µ—Ç (1/4): was a great day!\n",
            "  –ú–æ–¥–µ–ª—å (1/4): !!\n",
            "\n",
            "\n",
            "This is a really good thing for your time and I'm really happy to be back!\n",
            "Thank you everyone!\n",
            "I love\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä 3:\n",
            "  –í—Ö–æ–¥ (3/4):  i m playing guitar. l\n",
            "  –¢–∞—Ä–≥–µ—Ç (1/4): ove it.\n",
            "  –ú–æ–¥–µ–ª—å (1/4): m. m pf fj u fk fk fk fk fk fk fk fk fk fk fk\n",
            "\n",
            "‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ /content/text_autocomplete/results/transformer_metrics.json\n",
            "üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: /content/text_autocomplete/results/transformer_rouge.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content && zip -r text_autocomplete.zip text_autocomplete"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ8AtXHSdNqL",
        "outputId": "b951106d-658a-40c0-8db4-c269e339b041"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: text_autocomplete/ (stored 0%)\n",
            "  adding: text_autocomplete/src/ (stored 0%)\n",
            "  adding: text_autocomplete/src/lstm_model.py (deflated 53%)\n",
            "  adding: text_autocomplete/src/__pycache__/ (stored 0%)\n",
            "  adding: text_autocomplete/src/__pycache__/lstm_model.cpython-312.pyc (deflated 41%)\n",
            "  adding: text_autocomplete/src/__pycache__/next_token_dataset.cpython-312.pyc (deflated 42%)\n",
            "  adding: text_autocomplete/src/__pycache__/data_utils.cpython-312.pyc (deflated 45%)\n",
            "  adding: text_autocomplete/src/eval_transformer_pipeline.py (deflated 54%)\n",
            "  adding: text_autocomplete/src/data_utils.py (deflated 58%)\n",
            "  adding: text_autocomplete/src/eval_lstm.py (deflated 58%)\n",
            "  adding: text_autocomplete/src/next_token_dataset.py (deflated 58%)\n",
            "  adding: text_autocomplete/src/lstm_train.py (deflated 63%)\n",
            "  adding: text_autocomplete/results/ (stored 0%)\n",
            "  adding: text_autocomplete/results/ppl.png (deflated 10%)\n",
            "  adding: text_autocomplete/results/lstm_metrics_eval.json (deflated 41%)\n",
            "  adding: text_autocomplete/results/rouge_eval.png (deflated 20%)\n",
            "  adding: text_autocomplete/results/transformer_rouge.png (deflated 20%)\n",
            "  adding: text_autocomplete/results/transformer_metrics.json (deflated 18%)\n",
            "  adding: text_autocomplete/results/lstm_metrics.json (deflated 41%)\n",
            "  adding: text_autocomplete/results/loss.png (deflated 9%)\n",
            "  adding: text_autocomplete/artifacts/ (stored 0%)\n",
            "  adding: text_autocomplete/artifacts/vocab.json (deflated 68%)\n",
            "  adding: text_autocomplete/models/ (stored 0%)\n",
            "  adding: text_autocomplete/models/lstm.pt (deflated 8%)\n",
            "  adding: text_autocomplete/data/ (stored 0%)\n",
            "  adding: text_autocomplete/data/raw_dataset.csv (deflated 54%)\n",
            "  adding: text_autocomplete/data/tweets.txt (deflated 55%)\n",
            "  adding: text_autocomplete/data/train.csv (deflated 59%)\n",
            "  adding: text_autocomplete/data/test.csv (deflated 59%)\n",
            "  adding: text_autocomplete/data/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: text_autocomplete/data/val.csv (deflated 59%)\n",
            "  adding: text_autocomplete/data/dataset_processed.csv (deflated 59%)\n",
            "  adding: text_autocomplete/.ipynb_checkpoints/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–í—ã–≤–æ–¥—ã\n",
        "\n",
        "- –í —Ö–æ–¥–µ –ø—Ä–æ–µ–∫—Ç–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∏ –æ—Ü–µ–Ω–µ–Ω—ã –¥–≤–µ –º–æ–¥–µ–ª–∏:\n",
        "  1. **LSTM-LM**, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ —Ç–≤–∏—Ç–æ–≤\n",
        "  2. –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è **distilgpt2**, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è —á–µ—Ä–µ–∑ `transformers.pipeline` –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.  \n",
        "\n",
        "- LSTM –ø–æ–∫–∞–∑–∞–ª–∞ ROUGE-1 ‚âà 0.14 –∏ ROUGE-2 ‚âà 0.03 ‚Äî —ç—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –º–æ–¥–µ–ª—å —É–ª–æ–≤–∏–ª–∞ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∫–æ—Ä–ø—É—Å–∞.  \n",
        "- distilgpt2 –ø–æ–∫–∞–∑–∞–ª–∞ ROUGE-1 ‚âà 0.03 –∏ ROUGE-2 ‚âà 0.004, –Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏ –±–æ–ª–µ–µ —Å–≤—è–∑–Ω—ã–µ —Ñ—Ä–∞–∑—ã.  \n",
        "- –í —Ä–∞–º–∫–∞—Ö –∑–∞–¥–∞–Ω–∏—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ **–Ω–µ —Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å**; –æ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω ¬´–∏–∑ –∫–æ—Ä–æ–±–∫–∏¬ª –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–æ–¥—Ö–æ–¥–æ–≤.  \n",
        "**–í—ã–≤–æ–¥:** –æ–±—É—á–µ–Ω–Ω–∞—è LSTM –ª—É—á—à–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–æ—Ä–ø—É—Å–∞, –∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å–æ–∑–¥–∞—ë—Ç –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.  \n",
        "–ü–æ –º–æ–µ–º—É –º–Ω–µ–Ω–∏—é, –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –±—É–¥–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏–µ distilgpt2 –Ω–∞ —Å–≤–æ—ë–º –¥–∞—Ç–∞—Å–µ—Ç–µ.\n"
      ],
      "metadata": {
        "id": "OiGwghXkgp4s"
      }
    }
  ]
}